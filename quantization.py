# -*- coding: utf-8 -*-
"""quantization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-INiU_N_d5uQsD5nsTueSEaKRrXdfKY
"""

from tensorflow.keras.models import load_model

# Load the merged model
model = load_model('mobilenetv2_merged.keras')

# Print the model summary to see the layers and number of neurons
model.summary()

import numpy as np

# Load the merged model
model = load_model('mobilenetv2_merged.keras')

# Extract the Dense layers' weights and biases
dense_4_weights, dense_4_biases = model.layers[87].get_weights()  # Assuming dense_4 is at layer 87
dense_5_weights, dense_5_biases = model.layers[88].get_weights()  # Assuming dense_5 is at layer 88

# Quantize the weights and biases to INT16
def quantize_to_int16(tensor):
    scale = np.max(np.abs(tensor)) / 32767  # Scaling factor for INT16
    tensor_q = np.round(tensor / scale).astype(np.int16)  # Quantize and cast to INT16
    return tensor_q, scale

# Quantizing Dense layer 4
dense_4_weights_q, scale_w4 = quantize_to_int16(dense_4_weights)
dense_4_biases_q, scale_b4 = quantize_to_int16(dense_4_biases)

# Quantizing Dense layer 5 (Output Layer)
dense_5_weights_q, scale_w5 = quantize_to_int16(dense_5_weights)
dense_5_biases_q, scale_b5 = quantize_to_int16(dense_5_biases)

# Save the quantized weights and biases
np.save('dense_4_weights_int16.npy', dense_4_weights_q)
np.save('dense_4_biases_int16.npy', dense_4_biases_q)
np.save('dense_5_weights_int16.npy', dense_5_weights_q)
np.save('dense_5_biases_int16.npy', dense_5_biases_q)

# Save the scaling factors for later use in FPGA implementation
with open("quantization_scales.txt", "w") as f:
    f.write(f"scale_weights_4: {scale_w4}\n")
    f.write(f"scale_biases_4: {scale_b4}\n")
    f.write(f"scale_weights_5: {scale_w5}\n")
    f.write(f"scale_biases_5: {scale_b5}\n")

import numpy as np

# Load the weights and biases for dense_4 and dense_5
dense_4_weights = np.load('dense_4_weights_int16.npy')
dense_4_biases = np.load('dense_4_biases_int16.npy')
dense_5_weights = np.load('dense_5_weights_int16.npy')
dense_5_biases = np.load('dense_5_biases_int16.npy')

# Function to display neuron count and datatype
def check_neurons_and_dtype(weights, biases, layer_name):
    # Get number of neurons: the number of neurons is equal to the number of units in the Dense layer,
    # which is the number of rows in the weights matrix (assuming the input dimension is the first axis)
    num_neurons = weights.shape[1]  # The second dimension of the weights matrix represents the neurons in Dense layer

    # Get the datatype of weights and biases
    weights_dtype = weights.dtype
    biases_dtype = biases.dtype

    # Print the layer info
    print(f"Layer: {layer_name}")
    print(f"  Number of Neurons: {num_neurons}")
    print(f"  Weight Datatype: {weights_dtype}")
    print(f"  Bias Datatype: {biases_dtype}")
    print("-" * 50)

# Check Dense layer 4 (dense_4)
check_neurons_and_dtype(dense_4_weights, dense_4_biases, 'dense_4')

# Check Dense layer 5 (dense_5)
check_neurons_and_dtype(dense_5_weights, dense_5_biases, 'dense_5')